{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T05:39:48.125777Z",
     "start_time": "2024-07-23T05:39:20.333810Z"
    }
   },
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from vit_pytorch.efficient import ViT"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "glob: find all the pathnames matching a specified pattern\n",
    "\n",
    "import glob\n",
    "\n",
    "# Find all .txt files in the current directory\n",
    "files = glob.glob('*.txt')\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "        # You can now process the content of each file\n",
    "        print(f\"Content of {file}:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 40)"
   ],
   "id": "b4be40c2ef19977c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T05:46:05.999932Z",
     "start_time": "2024-07-23T05:46:05.990756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "a = [[1, 2], [3, 4]]\n",
    "b = ['a', 'b']\n",
    "for item in itertools.chain(b, a):\n",
    "    print(item)\n",
    "list(itertools.chain([1, 2], ['a', 'b']))\n"
   ],
   "id": "aa2daeffdf103fd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "[1, 2]\n",
      "[3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 'a', 'b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('example.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('extracted_folder')\n"
   ],
   "id": "80317a39523c6a7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Linformer: designed to reduce the complexity of Transformers from quadratic to linear\n",
    "\n",
    "from linformer import Linformer\n",
    "# Example usage of Linformer model initialization\n",
    "model = Linformer(\n",
    "    dim=512,\n",
    "    seq_len=128,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    k=256\n",
    ")"
   ],
   "id": "e15802a23202be02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sklearn.model_selection: splitting datasets, cross-validation, and hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "40c5e59c8a4dcbdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tqdm: useful for tracking the progress of loops and iterable processing. tqdm.notebook is specifically designed for Jupyter notebooks.\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    pass\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    pass\n",
    "\"\"\""
   ],
   "id": "cc7d85085c63f140"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T05:41:21.262476Z",
     "start_time": "2024-07-23T05:41:21.256112Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Torch: {torch.__version__}\")",
   "id": "3bf46cb3dc12cf7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.3.1+cpu\n",
      "hello world!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:57:07.945430Z",
     "start_time": "2024-07-23T08:57:07.940502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 4\n",
    "epochs = 2\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "seed = 42"
   ],
   "id": "154b950e12a0fd20",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:00:34.597852Z",
     "start_time": "2024-07-23T06:00:34.583766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ],
   "id": "57176dd5cfe5ee7b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#__init__\n",
    "\n",
    "from vit_pytorch.vit import ViT\n",
    "from vit_pytorch.simple_vit import SimpleViT\n",
    "\n",
    "from vit_pytorch.mae import MAE\n",
    "from vit_pytorch.dino import Dino"
   ],
   "id": "71a5b5752e2087c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:57:22.406550Z",
     "start_time": "2024-07-23T08:57:22.383426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#vit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ],
   "id": "d157746ff91c4739",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:57:10.929981Z",
     "start_time": "2024-07-23T08:57:10.918675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Efficient\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, transformer, pool = 'cls', channels = 3):\n",
    "        super().__init__()\n",
    "        image_size_h, image_size_w = pair(image_size)\n",
    "        assert image_size_h % patch_size == 0 and image_size_w % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        num_patches = (image_size_h // patch_size) * (image_size_w // patch_size)\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ],
   "id": "231ba7c8690b8754",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:59:41.515902Z",
     "start_time": "2024-07-23T07:59:41.490164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Dino\n",
    "import copy\n",
    "import random\n",
    "from functools import wraps, partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, default):\n",
    "    return val if exists(val) else default\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "# loss function # (algorithm 1 in the paper)\n",
    "\n",
    "def loss_fn(\n",
    "    teacher_logits,\n",
    "    student_logits,\n",
    "    teacher_temp,\n",
    "    student_temp,\n",
    "    centers,\n",
    "    eps = 1e-20\n",
    "):\n",
    "    teacher_logits = teacher_logits.detach()\n",
    "    student_probs = (student_logits / student_temp).softmax(dim = -1)\n",
    "    teacher_probs = ((teacher_logits - centers) / teacher_temp).softmax(dim = -1)\n",
    "    return - (teacher_probs * torch.log(student_probs + eps)).sum(dim = -1).mean()\n",
    "\n",
    "# augmentation utils\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "# exponential moving average\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "# MLP class for projector and predictor\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def forward(self, x, eps = 1e-6):\n",
    "        norm = x.norm(dim = 1, keepdim = True).clamp(min = eps)\n",
    "        return x / norm\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, dim_out, num_layers, hidden_size = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        dims = (dim, *((hidden_size,) * (num_layers - 1)))\n",
    "\n",
    "        for ind, (layer_dim_in, layer_dim_out) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "            is_last = ind == (len(dims) - 1)\n",
    "\n",
    "            layers.extend([\n",
    "                nn.Linear(layer_dim_in, layer_dim_out),\n",
    "                nn.GELU() if not is_last else nn.Identity()\n",
    "            ])\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            *layers,\n",
    "            L2Norm(),\n",
    "            nn.Linear(hidden_size, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# a wrapper class for the base neural network\n",
    "# will manage the interception of the hidden layer output\n",
    "# and pipe it into the projecter and predictor nets\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    def __init__(self, net, output_dim, projection_hidden_size, projection_num_layers, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.projector = None\n",
    "        self.projection_hidden_size = projection_hidden_size\n",
    "        self.projection_num_layers = projection_num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.hidden = {}\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, input, output):\n",
    "        device = input[0].device\n",
    "        self.hidden[device] = output.flatten(1)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    @singleton('projector')\n",
    "    def _get_projector(self, hidden):\n",
    "        _, dim = hidden.shape\n",
    "        projector = MLP(dim, self.output_dim, self.projection_num_layers, self.projection_hidden_size)\n",
    "        return projector.to(hidden)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        self.hidden.clear()\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden[x.device]\n",
    "        self.hidden.clear()\n",
    "\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, return_projection = True):\n",
    "        embed = self.get_embedding(x)\n",
    "        if not return_projection:\n",
    "            return embed\n",
    "\n",
    "        projector = self._get_projector(embed)\n",
    "        return projector(embed), embed\n",
    "\n",
    "# main class\n",
    "\n",
    "class Dino(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        image_size,\n",
    "        hidden_layer = -2,\n",
    "        projection_hidden_size = 256,\n",
    "        num_classes_K = 65336,\n",
    "        projection_layers = 4,\n",
    "        student_temp = 0.9,\n",
    "        teacher_temp = 0.04,\n",
    "        local_upper_crop_scale = 0.4,\n",
    "        global_lower_crop_scale = 0.5,\n",
    "        moving_average_decay = 0.9,\n",
    "        center_moving_average_decay = 0.9,\n",
    "        augment_fn = None,\n",
    "        augment_fn2 = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "        # default BYOL augmentation\n",
    "\n",
    "        DEFAULT_AUG = torch.nn.Sequential(\n",
    "            RandomApply(\n",
    "                T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "                p = 0.3\n",
    "            ),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            RandomApply(\n",
    "                T.GaussianBlur((3, 3), (1.0, 2.0)),\n",
    "                p = 0.2\n",
    "            ),\n",
    "            T.Normalize(\n",
    "                mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "                std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "        )\n",
    "\n",
    "        self.augment1 = default(augment_fn, DEFAULT_AUG)\n",
    "        self.augment2 = default(augment_fn2, DEFAULT_AUG)\n",
    "\n",
    "        # local and global crops\n",
    "\n",
    "        self.local_crop = T.RandomResizedCrop((image_size, image_size), scale = (0.05, local_upper_crop_scale))\n",
    "        self.global_crop = T.RandomResizedCrop((image_size, image_size), scale = (global_lower_crop_scale, 1.))\n",
    "\n",
    "        self.student_encoder = NetWrapper(net, num_classes_K, projection_hidden_size, projection_layers, layer = hidden_layer)\n",
    "\n",
    "        self.teacher_encoder = None\n",
    "        self.teacher_ema_updater = EMA(moving_average_decay)\n",
    "\n",
    "        self.register_buffer('teacher_centers', torch.zeros(1, num_classes_K))\n",
    "        self.register_buffer('last_teacher_centers',  torch.zeros(1, num_classes_K))\n",
    "\n",
    "        self.teacher_centering_ema_updater = EMA(center_moving_average_decay)\n",
    "\n",
    "        self.student_temp = student_temp\n",
    "        self.teacher_temp = teacher_temp\n",
    "\n",
    "        # get device of network and make wrapper same device\n",
    "        device = get_module_device(net)\n",
    "        self.to(device)\n",
    "\n",
    "        # send a mock image tensor to instantiate singleton parameters\n",
    "        self.forward(torch.randn(2, 3, image_size, image_size, device=device))\n",
    "\n",
    "    @singleton('teacher_encoder')\n",
    "    def _get_teacher_encoder(self):\n",
    "        teacher_encoder = copy.deepcopy(self.student_encoder)\n",
    "        set_requires_grad(teacher_encoder, False)\n",
    "        return teacher_encoder\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.teacher_encoder\n",
    "        self.teacher_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert self.teacher_encoder is not None, 'target encoder has not been created yet'\n",
    "        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n",
    "\n",
    "        new_teacher_centers = self.teacher_centering_ema_updater.update_average(self.teacher_centers, self.last_teacher_centers)\n",
    "        self.teacher_centers.copy_(new_teacher_centers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_embedding = False,\n",
    "        return_projection = True,\n",
    "        student_temp = None,\n",
    "        teacher_temp = None\n",
    "    ):\n",
    "        if return_embedding:\n",
    "            return self.student_encoder(x, return_projection = return_projection)\n",
    "\n",
    "        image_one, image_two = self.augment1(x), self.augment2(x)\n",
    "\n",
    "        local_image_one, local_image_two   = self.local_crop(image_one),  self.local_crop(image_two)\n",
    "        global_image_one, global_image_two = self.global_crop(image_one), self.global_crop(image_two)\n",
    "\n",
    "        student_proj_one, _ = self.student_encoder(local_image_one)\n",
    "        student_proj_two, _ = self.student_encoder(local_image_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_encoder = self._get_teacher_encoder()\n",
    "            teacher_proj_one, _ = teacher_encoder(global_image_one)\n",
    "            teacher_proj_two, _ = teacher_encoder(global_image_two)\n",
    "\n",
    "        loss_fn_ = partial(\n",
    "            loss_fn,\n",
    "            student_temp = default(student_temp, self.student_temp),\n",
    "            teacher_temp = default(teacher_temp, self.teacher_temp),\n",
    "            centers = self.teacher_centers\n",
    "        )\n",
    "\n",
    "        teacher_logits_avg = torch.cat((teacher_proj_one, teacher_proj_two)).mean(dim = 0)\n",
    "        self.last_teacher_centers.copy_(teacher_logits_avg)\n",
    "\n",
    "        loss = (loss_fn_(teacher_proj_one, student_proj_two) + loss_fn_(teacher_proj_two, student_proj_one)) / 2\n",
    "        return loss"
   ],
   "id": "d690b0a27a5e7b38",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtuple_of_tensors_to_tensor\u001B[39m(tuple_of_tensors):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m  torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(tuple_of_tensors), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m a  \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(a)\n",
      "\u001B[1;31mTypeError\u001B[0m: expected Tensor as element 0 in argument 0, but got str"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
